# Система оценки компонентов проекта

Система оценки использует подходы из LangChain для оценки качества работы LLM компонентов.

## Типы оценок

### 1. Response vs retrieved docs (для RAG участков)
Проверяет, соответствует ли ответ системы извлеченным документам. Помогает обнаружить галлюцинации - когда модель выдает информацию, которой нет в контексте.

**Используется для:**
- `_extract_appointment_info()` - извлечение информации о записи с использованием RAG
- `_verify_doctor_availability()` - проверка врача через Google Sheets

### 2. Response vs reference answer (для всех участков)
Сравнивает ответ системы с эталонным ответом. Измеряет точность выполнения задачи.

**Используется для:**
- `_correct_transcription()` - коррекция транскрипции
- `_format_as_dialogue()` - форматирование диалога
- `_extract_appointment_info()` - извлечение информации
- `_classify_call()` - классификация звонка

## Структура файлов

### `evaluator.py`
Основной модуль с классами для оценки:
- `Evaluator` - основной класс оценщика
- Методы для разных типов оценок

### `run_evaluation.py`
Скрипт для запуска оценки:
- Оценка на тестовом датасете (с эталонными данными)
- Оценка на реальных звонках (только RAG проверка)

## Использование

### Запуск оценки на тестовом датасете

```bash
python run_evaluation.py
```

Скрипт:
1. Создает тестовый датасет с транскрипциями и эталонными данными
2. Обрабатывает каждую транскрипцию через систему
3. Сравнивает результаты с эталонными данными
4. Сохраняет результаты в `evaluation_results.json`

### Оценка на реальном звонке

Скрипт также автоматически оценивает последний звонок из истории:
- Проверяет RAG компоненты на галлюцинации
- Не требует эталонных данных

## Формат результатов

### Оценка RAG извлечения

```json
{
  "overall_score": 0.95,
  "hallucination_check": {
    "metric": "response_vs_retrieved_docs",
    "score": 1,
    "reasoning": "Ответ полностью основан на фактах"
  },
  "accuracy_check": {
    "metric": "response_vs_reference",
    "score": 0.9,
    "reasoning": "Ответ почти полностью соответствует эталону"
  }
}
```

### Оценка других компонентов

```json
{
  "metric": "response_vs_reference",
  "score": 0.95,
  "accuracy": "полностью_правильно",
  "reasoning": "Ответ соответствует эталону"
}
```

## Метрики

### Score (0-1)
- **1.0**: Полностью правильный ответ
- **0.75**: В основном правильный, небольшие неточности
- **0.5**: Частично правильный, есть отличия
- **0.25**: Почти неверный
- **0.0**: Полностью неверный

### Accuracy (для response vs reference)
- `полностью_правильно` - score >= 0.9
- `в_основном_правильно` - score >= 0.75
- `частично_правильно` - score >= 0.5
- `почти_неправильно` - score >= 0.25
- `неправильно` - score < 0.25

## Добавление тестовых данных

Для добавления новых тестовых примеров отредактируйте функцию `create_test_dataset()` в `run_evaluation.py`:

```python
{
    "transcription": "Ваша транскрипция...",
    "reference_data": {
        "corrected_transcription": "Эталонная коррекция...",
        "formatted_transcription": "- Реплика 1\n- Реплика 2",
        "appointment_info": {
            "doctor_name": "...",
            # ... другие поля
        },
        "classification": {
            "type": "...",
            # ... другие поля
        }
    }
}
```

## Примеры использования

### Программное использование

```python
from evaluator import Evaluator

evaluator = Evaluator()

# Оценка соответствия извлеченным документам
result = evaluator.evaluate_response_vs_retrieved_docs(
    question="Извлеки информацию о записи",
    retrieved_docs="Контекст из Google Sheets...",
    response='{"doctor_name": "Иванов", ...}'
)

# Оценка соответствия эталонному ответу
result = evaluator.evaluate_response_vs_reference(
    question="Исправь транскрипцию",
    reference_answer="Эталонная исправленная транскрипция",
    response="Исправленная транскрипция от системы"
)
```

## Интерпретация результатов

### RAG участки
- **Hallucination score = 1**: Нет галлюцинаций, ответ основан на контексте
- **Hallucination score = 0**: Есть галлюцинации, ответ содержит информацию вне контекста
- **Accuracy score**: Точность извлечения информации

### Остальные участки
- **Score близко к 1**: Высокая точность выполнения задачи
- **Score близко к 0**: Низкая точность, требуется улучшение

## Требования

- Python 3.8+
- `openai` - для работы с LLM
- `python-dotenv` - для загрузки переменных окружения

Все зависимости уже включены в `requirements.txt`.
